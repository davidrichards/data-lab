{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* train/test/split\n",
    "* evaluation slips/review\n",
    "* Progress Bar\n",
    "* Picle and joblib\n",
    "* Minio\n",
    "* Storage + ObjectStoree\n",
    "* Model metadata\n",
    "* Treatment type + storage\n",
    "* Evaluation type + storage\n",
    "* Subject type + storage\n",
    "* Full processing loop with storage, versions, search\n",
    "* changelog/report centrally\n",
    "* fix documentation/flow (possibly upgrade nbdev)\n",
    "\n",
    "## Later\n",
    "\n",
    "* NLP\n",
    "* automl\n",
    "* unsupervised\n",
    "* semi-supervised\n",
    "* FastAI v2\n",
    "* manual pipelines\n",
    "* PySpark pipeline\n",
    "* Great Expectations\n",
    "* Kibana\n",
    "* SQS + Terraform\n",
    "* cache\n",
    "* Flask\n",
    "* Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop\n",
    "\n",
    "Training should be done on a loop: subjects and treatments all getting consistent model evaluation with results stored centrally.  Basically, create reproducible results quickly, automate the easy stuff.\n",
    "\n",
    "Here I gather a few utilities, practices, interfaces, and demonstrations to make this easier for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidrichards/codes/hydra/lab\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "pwd = %pwd\n",
    "if pwd.split('/')[-1] == 'nbs':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from lab.util.test_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def artifact_storage(*a, **kw):\n",
    "    \"\"\"Infer how the user intends to store\n",
    "    artifacts: models, versions, treatments,\n",
    "    and evaluation results.\"\"\"\n",
    "\n",
    "    def noop(subject, treatment, model, result): pass\n",
    "    return noop\n",
    "\n",
    "def generic_runner(subject, treatment, *a, **kw):\n",
    "    \"\"\"Naive concept. I have something smarter\n",
    "    running somewhere, but I'm not sure where it is.\"\"\"\n",
    "    storage = artifact_storage(*a, **kw)\n",
    "    model = None\n",
    "    result = treatment(subject)\n",
    "    return storage(subject, treatment, model, result)\n",
    "\n",
    "def runner(*a, **kw):\n",
    "    \"\"\"Infer the runner and its artifact storage.\"\"\"\n",
    "    return generic_runner\n",
    "\n",
    "def train_loop(subjects, treatments, *a, **kw):\n",
    "    \"\"\"The main training loop.\"\"\"\n",
    "\n",
    "    fn = runner(*a, **kw)\n",
    "\n",
    "    for subject in np.array(subjects):\n",
    "        for treatment in np.array(treatments):\n",
    "            fn(subject, treatment, *a, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.20247019 11.00103108  9.93195434]\n",
      "t1: 10.2\n",
      "t2: 11.2\n",
      "t1: 12.0\n",
      "t2: 13.0\n",
      "t1: 10.93\n",
      "t2: 11.93\n"
     ]
    }
   ],
   "source": [
    "# This is not normal...expecting to use models, fit, predict,\n",
    "# hyper parameters, etc.\n",
    "\n",
    "def t(name, n, subject):\n",
    "    result = round(subject + n, 2)\n",
    "    print(f\"{name}: {result}\")\n",
    "    return result\n",
    "\n",
    "def t1(subject): return t('t1', 1, subject)\n",
    "def t2(subject): return t('t2', 2, subject)\n",
    "\n",
    "subjects = stats.norm.rvs(size=3) + 10\n",
    "print(subjects)\n",
    "\n",
    "train_loop(subjects, [t1, t2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, garbage.\n",
    "\n",
    "There's a possibility that I'm going this way, but I did that in reverse. So, I started fixing some things by training some models. And, that's giving me something more useful.\n",
    "\n",
    "The following is still wrong, but better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MD5 stuff](https://docs.python.org/3/library/hashlib.html) is kind of cool, but I always forget a step and have to look it up:\n",
    "\n",
    "* convert input to a string\n",
    "* build a hashing object (use hashlib's tools)\n",
    "* update the hashing object with a utf-8-encoded string\n",
    "* create a hex digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_md5(*s):\n",
    "    s = '|'.join(s)\n",
    "    m = hashlib.md5()\n",
    "    m.update(str(s).encode('utf-8'))\n",
    "    return m.hexdigest()\n",
    "\n",
    "class ObjectStore:\n",
    "    \"\"\"Pass through extract from a set of ordered\n",
    "    stores, or get the data directly.\"\"\"\n",
    "    \n",
    "    # TODO: replace with configured values\n",
    "    LOCAL_CACHE = '/tmp'\n",
    "    \n",
    "    @classmethod\n",
    "    def call(cls, url, **kw):\n",
    "        return cls(**kw)(url)\n",
    "\n",
    "    def __init__(self, **kw):\n",
    "        self.kw = kw\n",
    "        \n",
    "    @property\n",
    "    def local_cache(self):\n",
    "        \"\"\"Return a path for the directory where data is cached.\"\"\"\n",
    "        if hasattr(self, '_local_cache'): return self._local_cache\n",
    "        self._local_cache = Path(self.kw.get('local_cache', self.LOCAL_CACHE))\n",
    "        return self._local_cache\n",
    "    \n",
    "    @property\n",
    "    def certs(self):\n",
    "        if hasattr(self, '_certs'): return self._certs\n",
    "        self._certs = self.kw.get('certs')\n",
    "        return self._certs\n",
    "    \n",
    "    @property\n",
    "    def allow_raise(self):\n",
    "        if hasattr(self, '_allow_raise'): return self._allow_raise\n",
    "        self._allow_raise = self.kw.get('allow_raise', False)\n",
    "        return self._allow_raise\n",
    "    \n",
    "    @property\n",
    "    def re_fetch(self):\n",
    "        \"\"\"When true, overwrite the local cache from the remote source.\"\"\"\n",
    "        if hasattr(self, '_re_fetch'): return self._re_fetch\n",
    "        self._re_fetch = self.kw.get('re_fetch', False)\n",
    "        return self._re_fetch\n",
    "    \n",
    "    def local_file(self, url):\n",
    "        \"\"\"Convert a filename to an MD5 hash and join it\n",
    "        to the local cache.\"\"\"\n",
    "        return self.local_cache / to_md5(url)\n",
    "    \n",
    "    def content_from_url(self, url):\n",
    "        \"\"\"Use requests to fetch content from a url\"\"\"\n",
    "        kw = {}\n",
    "        if not self.certs is None: kw['certs'] = self.certs\n",
    "        try:\n",
    "            s = requests.get(url, **kw).content\n",
    "            return s.decode('utf-8')\n",
    "        except:\n",
    "            if self.allow_raise: raise\n",
    "            return False\n",
    "        \n",
    "    def get_local_content(self, url):\n",
    "        \"\"\"Read content from the local cache\"\"\"\n",
    "        try:\n",
    "            if self.re_fetch: return False\n",
    "            local = self.local_file(url)\n",
    "            if not local.exists(): return False\n",
    "            with open(local, 'r') as f:\n",
    "                return f.read()\n",
    "        except:\n",
    "            if self.allow_raise: raise\n",
    "            return False\n",
    "        \n",
    "    def get_remote_content(self, url):\n",
    "        \"\"\"Read content from a remote path\"\"\"\n",
    "        try:\n",
    "            content = self.content_from_url(url)\n",
    "            local = self.local_file(url)\n",
    "            with open(local, 'w') as f:\n",
    "                if hasattr(content, 'decode'):\n",
    "                    f.write(content.decode('utf-8'))\n",
    "                else:\n",
    "                    f.write(content)\n",
    "            return content\n",
    "        except:\n",
    "            if self.allow_raise: raise\n",
    "            return False\n",
    "    \n",
    "    def local_or_remote(self, url):\n",
    "        \"\"\"Return a local version of the data\n",
    "        or get it, save it, and return it.\"\"\"\n",
    "        return (\n",
    "            self.get_local_content(url) or\n",
    "            self.get_remote_content(url)\n",
    "        )\n",
    "\n",
    "    def __call__(self, url):\n",
    "        return self.local_or_remote(url)\n",
    "    \n",
    "class ValuesFromUrl:\n",
    "    \"\"\"Convert CSV content into a 2-d NumPy array.\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def call(cls, url, **kw):\n",
    "        return cls(**kw)(url)\n",
    "    \n",
    "    def __init__(self, **kw):\n",
    "        self.kw = kw\n",
    "\n",
    "    def content(self, url):\n",
    "        \"\"\"Use ObjectStore to get CSV data.\"\"\"\n",
    "        return ObjectStore.call(url, **self.kw)\n",
    "    \n",
    "    def values(self, url):\n",
    "        \"\"\"Use Pandas to convert CSV to NumPy.\"\"\"\n",
    "        content = self.content(url)\n",
    "        return pd.read_csv(io.StringIO(content)).values\n",
    "    \n",
    "    def __call__(self, url):\n",
    "        return self.values(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = ObjectStore()\n",
    "assert ObjectStore(local_cache='local').local_cache == Path('local')\n",
    "assert subject.local_cache == Path(ObjectStore.LOCAL_CACHE)\n",
    "assert ObjectStore(certs='certs').certs == 'certs'\n",
    "root = subject.local_cache\n",
    "expected = root/to_md5('foo')\n",
    "assert subject.local_file('foo') == expected\n",
    "\n",
    "invalid_url = 'http://invalid.example.com/some_file'\n",
    "assert not subject.content_from_url(invalid_url)\n",
    "with check_raises(message=\"Can permit raises with `allow_raise=True`.\"):\n",
    "    ObjectStore(allow_raise=True).content_from_url(invalid_url)\n",
    "assert not ObjectStore(re_fetch=True, allow_raise=True).get_local_content(invalid_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ObjectStore can get data remotely, with certs. If it does that, it will store it locally. If the content is already stored locally, it returns that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_from_url(url, **kw):\n",
    "    \"\"\"Get the content from the ObjectStore\"\"\"\n",
    "    return ObjectStore.call(url, **kw)\n",
    "\n",
    "def dataframe_from_url(url, **kw):\n",
    "    \"\"\"Create a Pandas DataFrame from content.\"\"\"\n",
    "    content = ObjectStore.call(**kw)\n",
    "    return pd.read_csv(io.StringIO(content))\n",
    "\n",
    "def values_from_url(url, **kw):\n",
    "    \"\"\"Get the NumPy array from url\"\"\"\n",
    "    return ValuesFromUrl.call(url, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_list(keywords, keys, require_keys=False):\n",
    "    \"\"\"Filter a dictionary by a set of keys\"\"\"\n",
    "    if require_keys:\n",
    "        return {key:keywords[key] for key in keys}\n",
    "    return {key:keywords.get(key) for key in keys}\n",
    "\n",
    "def k_fold(**kw):\n",
    "    \"\"\"Use scikit-learn's KFold with some control\n",
    "    on reasonable defaults.\"\"\"\n",
    "    defaults = {'n_splits': 10}\n",
    "    kw = {**defaults, **kw}\n",
    "    return model_selection.KFold(**kw)\n",
    "\n",
    "def process_model(name, model, X, Y, storage={}, scoring='accuracy', **kw):\n",
    "    \"\"\"Process a model using K-Fold cross validation.\"\"\"\n",
    "    kfold = k_fold()\n",
    "    result = model_selection.cross_val_score(\n",
    "        model, X, Y,\n",
    "        cv=kfold, scoring=scoring\n",
    "    )\n",
    "    store(name, result, storage=storage)\n",
    "\n",
    "def store(name, result, storage={}):\n",
    "    \"\"\"Simple storage of treatment results.\"\"\"\n",
    "    storage[name] = result\n",
    "\n",
    "def plot_results(results, plot=None, title='Algorithm Comparison', **kw):\n",
    "    \"\"\"Create a box plot for each treatment in a results dictionary.\"\"\"\n",
    "    if plot is None: plot = plt\n",
    "\n",
    "    fig = plot.figure()\n",
    "    fig.suptitle(title)\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(list(results.values()))\n",
    "    ax.set_xticklabels(list(results.keys()))\n",
    "    plot.show()\n",
    "\n",
    "def cheap_loop(treatments, subjects, display=True, **kw):\n",
    "    \"\"\"Create models without hyperparameter fine tuning\n",
    "    to determine which algorithms show promise on a particular\n",
    "    dataset\"\"\"\n",
    "    storage = {}\n",
    "    for (name, model) in treatments:\n",
    "        for (x, y) in subjects:\n",
    "            process_model(name, model(), x, y, storage=storage)\n",
    "    if display: plot_results(storage, **kw)\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "array = values_from_url(url, names=names)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "treatments = [\n",
    "#     ('LR', LogisticRegression,\n",
    "    ('LDA', LinearDiscriminantAnalysis),\n",
    "    ('KNN', KNeighborsClassifier),\n",
    "    ('CART', DecisionTreeClassifier),\n",
    "    ('NB', GaussianNB),\n",
    "    ('SVM', SVC),\n",
    "]\n",
    "\n",
    "subjects = [\n",
    "    (X, Y)\n",
    "]\n",
    "\n",
    "cheap_loop(treatments, subjects);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cheap_loop is a quick-and-dirty approach to treating data (no hyperparameters are explored).\n",
    "\n",
    "LogisticRegression is throwing warnings, so I'm just skipping it for now.\n",
    "\n",
    "Could use some slips, combine some other work I've done, approach different scoring, quite a few other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = k_fold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understinding the interface with a bunch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00215793, 0.00212216, 0.00172114, 0.00156498, 0.00153494,\n",
       "        0.00160885, 0.00207496, 0.00267291, 0.0018549 , 0.00202274]),\n",
       " 'score_time': array([0.00048518, 0.0004189 , 0.00037098, 0.00033188, 0.00033593,\n",
       "        0.00038695, 0.00052595, 0.00047517, 0.00051618, 0.00048208]),\n",
       " 'test_score': array([0.7012987 , 0.80519481, 0.75324675, 0.68831169, 0.79220779,\n",
       "        0.76623377, 0.83116883, 0.84210526, 0.76315789, 0.80263158])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(LinearDiscriminantAnalysis(), X, Y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_splits': 10, 'shuffle': False, 'random_state': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00238276, 0.00277519, 0.00173688, 0.00163102, 0.00171423]),\n",
       " 'score_time': array([0.00046515, 0.00052285, 0.00040293, 0.00040483, 0.00040078]),\n",
       " 'test_score': array([0.77272727, 0.73376623, 0.74509804, 0.81045752, 0.77124183])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(LinearDiscriminantAnalysis(), X, Y, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_jobs determines how many CPUs to use.\n",
    "\n",
    "Having a wrapper around such a powerful wrapper is weird, but not everything is scikit-learn. So, using a PyTorch example, wrapping with these tools, could be a good justification or a wake up moment that this really is a bad idea (the way I'm abstracting it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00223303, 0.001647  , 0.00156474, 0.00202608, 0.00159311]),\n",
       " 'score_time': array([0.00059009, 0.00037313, 0.00039124, 0.00042391, 0.00035787]),\n",
       " 'test_score': array([0.77272727, 0.73376623, 0.74509804, 0.81045752, 0.77124183])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(LinearDiscriminantAnalysis(), X, Y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00357914, 0.00269413, 0.00189805, 0.00198722, 0.00244617]),\n",
       " 'score_time': array([0.00093508, 0.00041485, 0.00043488, 0.00039077, 0.00044012]),\n",
       " 'test_score': array([0.77272727, 0.73376623, 0.74509804, 0.81045752, 0.77124183])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(LinearDiscriminantAnalysis(), X, Y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.773, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.734, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.745, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.810, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.771, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00295711, 0.00271225, 0.00283504, 0.00302076, 0.0023067 ]),\n",
       " 'score_time': array([0.00070405, 0.00080085, 0.000525  , 0.00055718, 0.000736  ]),\n",
       " 'test_score': array([0.77272727, 0.73376623, 0.74509804, 0.81045752, 0.77124183])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(LinearDiscriminantAnalysis(), X, Y, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so verbosity is kind of cool. The docs didn't show me what I could more easily just check here. I can get information about the concurrency, or see each job visually while it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00212193, 0.00183487, 0.00171804, 0.00200081, 0.00185704]),\n",
       " 'score_time': array([0.00050616, 0.00043225, 0.00050998, 0.00048423, 0.00046372]),\n",
       " 'test_score': array([0.77272727, 0.73376623, 0.74509804, 0.81045752, 0.77124183]),\n",
       " 'train_score': array([0.78303426, 0.76998369, 0.77850163, 0.76872964, 0.78501629])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(LinearDiscriminantAnalysis(), X, Y, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00239301, 0.00206709, 0.00233579, 0.00210571, 0.00190902]),\n",
       " 'score_time': array([0.00050688, 0.00049472, 0.00048828, 0.00059414, 0.00046802]),\n",
       " 'estimator': (LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                             solver='svd', store_covariance=False, tol=0.0001),\n",
       "  LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                             solver='svd', store_covariance=False, tol=0.0001),\n",
       "  LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                             solver='svd', store_covariance=False, tol=0.0001),\n",
       "  LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                             solver='svd', store_covariance=False, tol=0.0001),\n",
       "  LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                             solver='svd', store_covariance=False, tol=0.0001)),\n",
       " 'test_score': array([0.77272727, 0.73376623, 0.74509804, 0.81045752, 0.77124183])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(LinearDiscriminantAnalysis(), X, Y, return_estimator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at this point I've got some comfort around what works, or at least how this works. I'm a little loose on scoring (I should explore and grok that much better).\n",
    "\n",
    "Working in this way is worth 10 articles. There is no replacement for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77272727, 0.73376623, 0.74509804, 0.81045752, 0.77124183])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_val_score(LinearDiscriminantAnalysis(), X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's nice about cross_val_score is it has mostly the same interface as cross_validate, but it only returns the scoring. If I'm not going to keep the fit time and score time, then I can just ignore that.\n",
    "\n",
    "What I'm starting to think is I should normally keep the fit time. I also want a predict time and a develop time. It takes 3 days to build a model, say, and 10 minutes to train it and the resulting model can run in 100 milliseconds. Knowing that, I can spend 5 days building something that trains in 2 hours but returns either better results, or faster results, or some reason to have it.\n",
    "\n",
    "These are the values needed in the lab and in production. There are always a lot of metrics flying around, and developing confidence with what they mean and what to do needs to be translated into an experiential flow instead of a reflective stall.\n",
    "\n",
    "--\n",
    "\n",
    "At this point I'm thinking about abstraction again. What should I use? Do I want kfold?\n",
    "\n",
    "It's not as important as I made it.\n",
    "\n",
    "Do I want to go to a pandas dataframe?\n",
    "\n",
    "Maybe for now, but once I get a better sense of extract X, y, then maybe not so much.\n",
    "\n",
    "Why didn't you use tools for splitting the data?\n",
    "\n",
    "I think they had the cross validation, that those needed to be static. It makes assumptions about the data too, that they're independently and randomly distributed in the dataset. If I didn't have that, I should have split the data with tools.\n",
    "\n",
    "OK, so this is useful:\n",
    "\n",
    "* abstraction is off, can work better\n",
    "* grok is off (I'm shooting from the hip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* [emulating containers](https://docs.python.org/3/reference/datamodel.html?emulating-container-types#emulating-container-types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_matches(r, s):\n",
    "    \"\"\"Return all matches, or an empty list.\"\"\"\n",
    "    result = re.search(r, s)\n",
    "    if result is None: return []\n",
    "    return list(result.groups())\n",
    "\n",
    "def get_nth_match(r, s, n):\n",
    "    \"\"\"Search the string for a regular expression.\n",
    "    If there is a match, return the nth value(s).\n",
    "    \"\"\"\n",
    "    found = get_matches(r, s)\n",
    "    try:\n",
    "        return found[n]\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def get_first_match(r, s):\n",
    "    \"\"\"Shorthand for get_nth_match.\"\"\"\n",
    "    return get_nth_match(r, s, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r'(.+), (.+), and (.+)'\n",
    "s = \"a, b, and c\"\n",
    "assert get_nth_match(r, s, slice(0,2)) == list('ab')\n",
    "\n",
    "r = r'test_(.+)'\n",
    "s = 'test_accuracy'\n",
    "assert get_matches(r, s) == ['accuracy']\n",
    "assert get_first_match(r, s) == 'accuracy'\n",
    "\n",
    "r = r'foo_bar'\n",
    "s = 'No match'\n",
    "assert get_first_match(r, s) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Able to extract keys out of strings, using a regular expression without exposing the internal re quirks.\n",
    "\n",
    "If I know what I'm doing, I can use a slice.\n",
    "\n",
    "This comes from a [StackOverflow question](https://stackoverflow.com/questions/15340582/python-extract-pattern-matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementVersion:\n",
    "    \"\"\"Increment a version string.\n",
    "    There are packages for this (bumpversion) and\n",
    "    package-management tools, but I thought this\n",
    "    would be easier than it was.\"\"\"\n",
    "    \n",
    "    DEFAULT_VERSION = '0.0.0'\n",
    "    LEVELS = ['major', 'minor', 'patch']\n",
    "    DEFAULT_LEVEL = 'patch'\n",
    "    ZEROS = [0, 0, 0]\n",
    "    \n",
    "    @classmethod\n",
    "    def call(cls, version, **kw):\n",
    "        return cls()(version, **kw)\n",
    "        \n",
    "    def split_version(self, version):\n",
    "        try:\n",
    "            return [int(e) for e in str(version).split('.')]\n",
    "        except:\n",
    "            return self.ZEROS\n",
    "        \n",
    "    def get_version_parts(self, version):\n",
    "        \"\"\"Get a 3-piece version\"\"\"\n",
    "        values = self.split_version(version)[:3]\n",
    "        values = np.append(values, self.ZEROS)\n",
    "        return values[:3]\n",
    "    \n",
    "    def increment_version(self, version, level='patch', value=None, **kw):\n",
    "        \"\"\"Increment the major, minor, or patch.\n",
    "        Pass in the value, if an explicit value is needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not level in self.LEVELS: level = self.DEFAULT_LEVEL\n",
    "        level_index = self.LEVELS.index(level)\n",
    "        parts = self.get_version_parts(version)\n",
    "        results = []\n",
    "        for i, e in enumerate(parts):\n",
    "            if i < level_index:\n",
    "                results.append(str(e))\n",
    "            elif i == level_index:\n",
    "                if value is None: value = e + 1\n",
    "                results.append(str(value))\n",
    "            else:\n",
    "                results.append('0')\n",
    "                \n",
    "        return '.'.join(results)\n",
    "\n",
    "    def __call__(self, version, **kw):\n",
    "        return self.increment_version(version, **kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert IncrementVersion.call('0.0.1') == '0.0.2'\n",
    "assert IncrementVersion.call('') == '0.0.1'\n",
    "assert IncrementVersion.call('', level='major') == '1.0.0'\n",
    "assert IncrementVersion.call('0.1.1', level='minor') == '0.2.0'\n",
    "assert IncrementVersion.call('1.1.1.1', level='flurb') == '1.1.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work, with a missing version, different values, nonsense inputs, too-long inputs...it wasn't worth it writing my own, but there you have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    TEST_RE = r'test_(.+)'\n",
    "    DEFAULT_VERSION = '0.0.1'\n",
    "    DEFAULT = dict(\n",
    "        version=DEFAULT_VERSION,\n",
    "        fit_params={},\n",
    "        tests={},\n",
    "        fit_time=0,\n",
    "        trained_at=None\n",
    "    )\n",
    "    \n",
    "    def __init__(self, **kw):\n",
    "        self.kw = kw\n",
    "        self.records = {}\n",
    "        self.archive = []\n",
    "        \n",
    "    def __len__(self): return len(self.records)\n",
    "    def __length_hint(self): return self.__len__()\n",
    "    def __getitem__(self, key): return self.records.get(key)\n",
    "    \n",
    "    def _retire(self, key):\n",
    "        \"\"\"If it exists, retire the old version of a record.\n",
    "        Return the version of that record.\n",
    "        Otherwise return False.\n",
    "        Either way, this value can be incremented to get a new\n",
    "        version.\"\"\"\n",
    "        old = self.records.pop(key,None)\n",
    "        if old is None: return False\n",
    "        old['model_name'] = key\n",
    "        self.archive.append(old)\n",
    "        return old.get('version')\n",
    "    \n",
    "    def _get_version(self, key):\n",
    "        \"\"\"Retire an old record if it exists and increment the version.\"\"\"\n",
    "        old_version = self._retire(key)\n",
    "        return IncrementVersion.call(old_version)\n",
    "    \n",
    "    def _get_default(self, key):\n",
    "        \"\"\"Merge the best ideas for a default record.\"\"\"\n",
    "        version = self._get_version(key)\n",
    "        record = {\n",
    "            **self.DEFAULT,\n",
    "            **{'version': version},\n",
    "            **self.records.get(key, {})\n",
    "        }\n",
    "\n",
    "        if record.get('trained_at') is None:\n",
    "            record['trained_at'] = datetime.utcnow()\n",
    "\n",
    "        return record\n",
    "        \n",
    "    def __setitem__(self, key, value):\n",
    "        record = self._get_default(key)\n",
    "        value = dict(value)\n",
    "        for k, v in value.items():\n",
    "            name = get_first_match(self.TEST_RE, k)\n",
    "            if name is None:\n",
    "                record[k] = v\n",
    "            else:\n",
    "                record['tests'][name] = v\n",
    "        self.records[key] = record\n",
    "        return record\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo_data(url=None):\n",
    "    \"\"\"Return a simple/reusable dataset (y, X).\n",
    "    TODO: Use a more-generic splitting technique.\"\"\"\n",
    "    if url is None:\n",
    "        url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "    array = ValuesFromUrl.call(url)\n",
    "    return array[:,8], array[:,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = get_demo_data()\n",
    "assert np.shape(y) == (767,)\n",
    "assert np.shape(X) == (767, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = Storage()\n",
    "assert str(subject) == str({}) # Basic repr\n",
    "assert subject['not_found'] is None # Safe/simpler to get\n",
    "\n",
    "r1 = model_selection.cross_validate(LinearDiscriminantAnalysis(), X, y)\n",
    "subject['LDA'] = r1\n",
    "assert subject['LDA']['version'] == '0.0.1'\n",
    "assert subject['LDA']['fit_params'] == Storage.DEFAULT['fit_params']\n",
    "lda_tests = subject['LDA']['tests']\n",
    "assert all(lda_tests['score'] == r1['test_score'])\n",
    "assert 'trained_at' in subject['LDA']\n",
    "assert 'score_time' in subject['LDA']\n",
    "\n",
    "r2_now = datetime.utcnow()\n",
    "r2 = model_selection.cross_validate(\n",
    "    LinearDiscriminantAnalysis(), X, y,\n",
    "    scoring=['accuracy', 'f1']\n",
    ")\n",
    "r2['trained_at'] = r2_now\n",
    "subject['LDA'] = r2\n",
    "archived = subject.archive[-1]\n",
    "assert archived['version'] == '0.0.1'\n",
    "assert archived['model_name'] == 'LDA'\n",
    "assert subject['LDA']['version'] == '0.0.2'\n",
    "assert subject['LDA']['trained_at'] == r2_now\n",
    "assert 'accuracy' in subject['LDA']['tests']\n",
    "assert 'f1' in subject['LDA']['tests']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage is kind of cool at this point. It allows me to treat storage like it's a dictionary, meaning it's a cheap replacement for an in-memory tool. It adds some versioning, timestamping, archiving, and basically invents an easy-to-mock interface for storage. Later, this will wrap something like MinIO. That might not be 100% appropriate, but I want to ease into something I can understand/mock/extend.\n",
    "\n",
    "From here, I'd like to add some new wraps around the basic processing. Replace what's above with similar below. Specifically:\n",
    "\n",
    "**Treatments** I want to create some defaults around treatments that just work. This is for baseline models, classification, regression, unsupervised, semi-supervised, NLP, deep learning--all of it. Make it easy to find and pick what's normal in a situation.\n",
    "\n",
    "**Slips** I want to put more slips in my notebooks. I want to store/find/reuse these as documentation. I'm making a notebook a multi-step production, which slows it down. Let me just drop a `#slip` comment at the top of a markdown cell and make that searchable. I'll include full references in those cells.\n",
    "\n",
    "**Evaluation** I want to develop transparency around evaluation. What's common? What can I reuse? Again, I'm dealing with all the kinds of models and analysis here. Not everything applies to everything, but I want to have a quick way of seeing that good work is consistently done, or the most-obvious thing is to do the best practice first. The idea is that I want to train models with my best ideas, learn better ones, and start evaluating in better ways later, coming back to earlier models if I choose to.\n",
    "\n",
    "**Processing** Make the process just work: subjects and treatments, evaluations and storage. Create a learning loop.\n",
    "\n",
    "**Storage** Storage isn't quite done yet. Yes, there's the permanent version of this, an object storage, likely in MinIO. Also, there's storing models. Let me move those easily into production once they're trained, evaluated, tuned, and approved-of here. This suggests an interface change, that I was probably naive with what I did above on Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "    # {'fit_time': array([0.00210023, 0.00220704, 0.00217891, 0.00219107, 0.00271726]),\n",
    "    #  'score_time': array([0.00784898, 0.00204897, 0.00181198, 0.00148892, 0.00190783]),\n",
    "    #  'test_accuracy': array([0.77272727, 0.74025974, 0.74025974, 0.81045752, 0.77777778]),\n",
    "    #  'test_balanced_accuracy': array([0.72703704, 0.71055556, 0.68074074, 0.77075472, 0.72358491])}\n",
    "\n",
    "\n",
    "\n",
    "    def k_fold(**kw):\n",
    "        \"\"\"Use scikit-learn's KFold with some control\n",
    "        on reasonable defaults.\"\"\"\n",
    "        defaults = {'n_splits': 10}\n",
    "        kw = {**defaults, **kw}\n",
    "        return model_selection.KFold(**kw)\n",
    "\n",
    "    def process_model(name, model, X, Y, storage={}, scoring='accuracy', **kw):\n",
    "        \"\"\"Process a model using K-Fold cross validation.\"\"\"\n",
    "        kfold = k_fold()\n",
    "        result = model_selection.cross_val_score(\n",
    "            model, X, Y,\n",
    "            cv=kfold, scoring=scoring\n",
    "        )\n",
    "        return store(name, result, storage=storage)\n",
    "\n",
    "    def store(name, result, storage={}):\n",
    "        \"\"\"Simple storage of treatment results.\"\"\"\n",
    "        storage[name] = result\n",
    "        return storage\n",
    "\n",
    "    def plot_results(results, plot=None, title='Algorithm Comparison', **kw):\n",
    "        \"\"\"Create a box plot for each treatment in a results dictionary.\"\"\"\n",
    "        if plot is None: plot = plt\n",
    "\n",
    "        fig = plot.figure()\n",
    "        fig.suptitle(title)\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.boxplot(list(results.values()))\n",
    "        ax.set_xticklabels(list(results.keys()))\n",
    "        plot.show()\n",
    "\n",
    "    def cheap_loop(treatments, subjects, display=True, **kw):\n",
    "        \"\"\"Create models without hyperparameter fine tuning\n",
    "        to determine which algorithms show promise on a particular\n",
    "        dataset\"\"\"\n",
    "        storage = {}\n",
    "        for (name, model) in treatments:\n",
    "            for (x, y) in subjects:\n",
    "                process_model(name, model(), x, y, storage=storage)\n",
    "        if display: plot_results(storage, **kw)\n",
    "        return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
