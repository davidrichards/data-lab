{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lab.imports import *\n",
    "import lab.train.protocols\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.local_object_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Object Store\n",
    "\n",
    "Namespace for these functions. I'm trying a functional approach instead of wrapping everything in a one-type-use class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "WHITE_LIST_CONFIG = ['root', 'name', 'filter', 'force']\n",
    "\n",
    "def white_list(keywords, keys, require_keys=False, expand_missing=False):\n",
    "    \"\"\"Filter a dictionary by a set of keys\"\"\"\n",
    "    if require_keys:\n",
    "        return {key:keywords[key] for key in keys}\n",
    "    if expand_missing:\n",
    "        return {key:keywords.get(key) for key in keys}\n",
    "    result = {}\n",
    "    for key in keys:\n",
    "        if key in keywords: result[key] = keywords[key]\n",
    "    return result\n",
    "\n",
    "def merge_config(**kw):\n",
    "    \"\"\"Get the local environemnt variables, convert the keys\n",
    "    to lower case, merge new keywords over the environment\n",
    "    variables, and use a white list filter on valid entries.\n",
    "    \n",
    "    Right now, I'm only white listing the environment variables,\n",
    "    but possibly I'll do this for everything.\"\"\"\n",
    "    \n",
    "    env = {k.lower():v for k, v in os.environ.items()}\n",
    "    env = white_list(env, WHITE_LIST_CONFIG)\n",
    "    return {**env, **kw}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* Move the above into imports\n",
    "* Move what I have in imports into a notebook\n",
    "* Create a consistent set of configuration expectations (keys that work with local, minio, some of the ops)\n",
    "\n",
    "All of this should be done in a notebook first, because there are 100 things to test.\n",
    "\n",
    "More-specifically:\n",
    "\n",
    "* read an environment file, not settings, not .env\n",
    "* read .env\n",
    "* manage white lists and merges\n",
    "* create a constant for the configuration\n",
    "* ensure nothing sensitive is being shared (.gitignore, cached in a notebook)\n",
    "* make sure I have easy access to namespaced configuration\n",
    "* create a default set of parameters for basic ops (object store, at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(a=1, b=2)\n",
    "\n",
    "assert 'b' not in white_list(d, ['a'])\n",
    "assert 'c' in white_list(d, ['a', 'c'], expand_missing=True)\n",
    "assert white_list(d, ['c'], expand_missing=True)['c'] is None\n",
    "\n",
    "with check_raises():\n",
    "    white_list(d, ['c'], require_keys=True)\n",
    "    \n",
    "known_keys = ['force', 'filter', 'name', 'root']\n",
    "for key in known_keys:\n",
    "    assert key in WHITE_LIST_CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, to bring in the environment variables. It's because in a Docker container, this is the best way to handle configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "ROOT = '/tmp'\n",
    "def _root(**kw):\n",
    "    \"\"\"Expose root from the config or a default.\"\"\"\n",
    "    kw = merge_config(**kw)\n",
    "    return Path(kw.get('root', ROOT))\n",
    "\n",
    "def _is_re(o):\n",
    "    \"\"\"Deal with whether an object is a regular expression.\n",
    "    Modern Python uses re.Pattern for all compiled regular\n",
    "    expressions, but in 3.6.9 and ostensibly before, I don't\n",
    "    have this. So, duck type it: if it has match and pattern,\n",
    "    good enough.\"\"\"\n",
    "    return hasattr(o, 'match') and hasattr(o, 'pattern')\n",
    "\n",
    "def _name(**kw):\n",
    "    \"\"\"Name is gathered from the configuration or keyword\n",
    "    arguments. It uses name or filter, because both keys\n",
    "    make sense with different calls. Name is used\n",
    "    for the bucket or item name, depending on the context.\"\"\"\n",
    "    kw = merge_config(**kw)\n",
    "    o = kw.get('name', kw.get('filter', None))\n",
    "    if _is_re(o):\n",
    "        return o.pattern\n",
    "    if o is None or isinstance(o, str):\n",
    "        return o\n",
    "    return str(o)\n",
    "\n",
    "def _bucket_filter(**kw):\n",
    "    kw = merge_config(**kw)\n",
    "    o = kw.get('name', kw.get('filter', ''))\n",
    "    if _is_re(o): return o\n",
    "    return re.compile(f\".*{o}.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _root(root='a') == Path('a')\n",
    "assert _root() == Path(ROOT)\n",
    "\n",
    "assert _name() is None\n",
    "assert _name(name='name') == 'name'\n",
    "assert _name(filter='name') == 'name'\n",
    "r = re.compile('name')\n",
    "assert _name(name=r) == 'name'\n",
    "assert _name(filter=r) == 'name'\n",
    "assert _name(name=42) == '42'\n",
    "assert _name(filter=42) == '42'\n",
    "\n",
    "assert _bucket_filter().pattern == '.*.*'\n",
    "assert _bucket_filter().match('')\n",
    "assert _bucket_filter().match('anything')\n",
    "assert _bucket_filter(name='foo').match('foo')\n",
    "assert _bucket_filter(name=re.compile('foo')).match('foo')\n",
    "assert _bucket_filter(filter='foo').match('foo')\n",
    "assert _bucket_filter(filter=re.compile('foo')).match('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting root, a name, and a filter for buckets. This is mostly just establishing a convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap between bucket/directory and item/file is a little weird. I opaquely thought that types would begin to straighten that out. Given a type of a thing, when I put it, I can find or create an appropriate bucket for it, given a mapping.\n",
    "\n",
    "There's a bit of weirdness going on with:\n",
    "\n",
    "* filter/name being top-level\n",
    "* calling keyword extrapolation several times for a single call\n",
    "* from opaque-to-specific on something like a bucket or a type of thing to enforce\n",
    "\n",
    "The idea is I get a set of subject references, say from a filter or something like ['research', 'supervised', 'small']. Then, I get a set of treatments like, ['common', 'first pass']. Then I look for or create each model, allowing for a comparative replacement to be at least stored in memory. I train the models, evaluate them, possbly label, version, and store them. Maybe the treatment can fill in the invocation gaps, if I need those. Maybe I run some post-training diagnostics on subjects and models for expectations.\n",
    "\n",
    "A lot of this is finding, organizing, and creating objects. There are fundamental issues with the above and below, so what I'm going to do is create a fake set of sequences, see how to work it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataset Name\tBrief description\tPreprocessing\tInstances\tFormat\tDefault Task\tCreated (updated)\tReference\tCreator\n",
    "\n",
    "Bike Sharing Dataset\tHourly and daily count of rental bikes in a large city.\tMany features, including weather, length of trip, etc., are given.\t17,389\tText\tRegression\t2013\t[427][428]\tH. Fanaee-T\n",
    "\n",
    "New York City Taxi Trip Data\tTrip data for yellow and green taxis in New York City.\tGives pick up and drop off locations, fares, and other details of trips.\t6 years\tText\tClassification, clustering\t2015\t[429]\tNew York City Taxi and Limousine Commission\n",
    "\n",
    "Taxi Service Trajectory ECML PKDD\tTrajectories of all taxis in a large city.\tMany features given, including start and stop points.\t1,710,671\tText\tClustering, causal-discovery\t2015\t[430][431]\tM. Ferreira et al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_subjects_raw = [\n",
    "    dict(\n",
    "        name='bike',\n",
    "        short_description='Bike Sharing Dataset',\n",
    "        description='Hourly and daily count of rental bikes in a large city.',\n",
    "        preprocessing='Many features, including weather, length of trip, etc., are given.',\n",
    "        instances=17389,\n",
    "        format='text',\n",
    "        created_on=dict(year=2013),\n",
    "        url='tbd',\n",
    "        contributors=['H. Fanaee-T'],\n",
    "        tags=['regression', 'medium', 'supervised'],\n",
    "    ),\n",
    "    dict(name='test'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab.train.protocols as protocol_utils\n",
    "\n",
    "def _dict_from_name(o):\n",
    "    if isinstance(o, dict): return d\n",
    "    return {'name': o}\n",
    "\n",
    "def _list_of_dicts(l):\n",
    "    return [_dict_from_name(e) for e in l]\n",
    "\n",
    "def _value_format(o):\n",
    "    if isinstance(o, list): return _list_of_dicts(o)\n",
    "    return o\n",
    "\n",
    "def _raw_format(d):\n",
    "    return {k:_value_format(v) for k, v in d.items()}\n",
    "\n",
    "def _see_saw(d, builder):\n",
    "    message = builder(**d)\n",
    "    d = protocol_utils.to_dict(message)\n",
    "    return builder(**d)\n",
    "\n",
    "def _raw_to_prototype(raw, kind):\n",
    "    d = _raw_format(raw)\n",
    "    builder = protocol_utils.MessageBuilder(kind=kind)\n",
    "    return _see_saw(d, builder)\n",
    "#     return builder(**d)\n",
    "\n",
    "def _list_to_prototypes(l, kind):\n",
    "    return [_raw_to_prototype(raw, kind) for raw in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"bike\"\n",
       "type: \"supervised\"\n",
       "size: \"medium\"\n",
       "short_description: \"Bike Sharing Dataset\"\n",
       "description: \"Hourly and daily count of rental bikes in a large city.\"\n",
       "preprocessing: \"Many features, including weather, length of trip, etc., are given.\"\n",
       "instances: 17389\n",
       "format: \"text\"\n",
       "created_on {\n",
       "  year: 2013\n",
       "}\n",
       "url: \"tbd\"\n",
       "version {\n",
       "}\n",
       "contributors {\n",
       "  name: \"H. Fanaee-T\"\n",
       "}\n",
       "tags {\n",
       "  name: \"regression\"\n",
       "}\n",
       "tags {\n",
       "  name: \"medium\"\n",
       "}\n",
       "tags {\n",
       "  name: \"supervised\"\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = _list_to_prototypes(_subjects_raw, 'Subject')\n",
    "assert isinstance(generated, list)\n",
    "message = generated[0]\n",
    "assert type(message) == training_prototypes.Subject\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively figuring out what should come out of the object store, how best to access it, what the learning loop needs to do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected at least 1, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-624f65a5a314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected at least 1, got 0)"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "h, *t = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_PREFIX_MAP = dict(\n",
    "    subject = ['type', 'size', 'name', 'year', 'month', 'version'],\n",
    "    treatment = ['type', 'name'],\n",
    "    model = ['treatment', 'name', 'type'],\n",
    "    evaluation = ['type', 'name', 'year', 'month'],\n",
    "    expectation = ['source_type', 'name', 'year', 'month'],\n",
    "    invocation = ['name', 'version'],\n",
    ")\n",
    "\n",
    "def _find_on_message(message, attribute_path):\n",
    "    if len(attribute_path) == 0: return message\n",
    "    if len(attribute_path) == 1:\n",
    "        attribute = attribute_path[0]\n",
    "        if hasattr(message, attribute):\n",
    "            return getattr(message, attribute)\n",
    "        return None\n",
    "    attribute, *attribute_path = attribute_path\n",
    "    if hasattr(message, attribute):\n",
    "        message = getattr(message, attribute)\n",
    "        return _find_on_message(message, attribute_path)\n",
    "    \n",
    "def _created_on_year(message):\n",
    "    return _find_on_message(message, ['created_on', 'year'])\n",
    "\n",
    "def _created_on_month(message):\n",
    "    return _find_on_message(message, ['created_on', 'month'])\n",
    "\n",
    "def _version_string(message):\n",
    "    version = _find_on_message(message, ['version'])\n",
    "    if version is None: return None\n",
    "    return f\"{version.major}.{version.minor}.{version.patch}\"\n",
    "\n",
    "FIELD_TYPE_MAP = dict(\n",
    "    year = _created_on_year,\n",
    "    month = _created_on_month,\n",
    "    version = _version_string,\n",
    ")\n",
    "NAME_DELIMITER = \"|\"\n",
    "\n",
    "def _value_from_message(message, key):\n",
    "    if key in FIELD_TYPE_MAP:\n",
    "        value = FIELD_TYPE_MAP[key](message)\n",
    "    else:\n",
    "        value = _find_on_message(message, [key])\n",
    "    if value is None: return ''\n",
    "    return str(value)\n",
    "\n",
    "def _build_until_missing(keys, d):\n",
    "    result = []\n",
    "    for key in keys:\n",
    "        if not key in d:\n",
    "            break\n",
    "        result.append(str(d[key]))\n",
    "    return result\n",
    "\n",
    "def _prefix_from_dictionary(d, kind):\n",
    "    if kind not in TYPE_PREFIX_MAP: return None\n",
    "    keys = TYPE_PREFIX_MAP[kind]\n",
    "    values = _build_until_missing(keys, d)\n",
    "    return NAME_DELIMITER.join(values)\n",
    "\n",
    "def _key_from_message(message, kind):\n",
    "    if kind not in TYPE_PREFIX_MAP: return None\n",
    "    keys = TYPE_PREFIX_MAP[kind]\n",
    "    values = [_value_from_message(message, key) for key in keys]\n",
    "    return NAME_DELIMITER.join(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_to_dict(l):\n",
    "    \"\"\"Test helper: key == value, so expectations are obvious.\"\"\"\n",
    "    return {k:k for k in l}\n",
    "\n",
    "assert _prefix_from_dictionary({}, 'weird') is None\n",
    "assert _prefix_from_dictionary(_list_to_dict(['a', 'b']), 'subject') == ''\n",
    "assert _prefix_from_dictionary(\n",
    "    _list_to_dict(['type', 'size', 'name', 'month']), 'subject'\n",
    ") == 'type|size|name'\n",
    "assert _prefix_from_dictionary(\n",
    "    _list_to_dict(['type', 'name', 'foo']), 'treatment'\n",
    ") == 'type|name'\n",
    "\n",
    "message = _raw_to_prototype(_subjects_raw[0], 'Subject')\n",
    "message\n",
    "\n",
    "assert _key_from_message(message, 'weird') is None\n",
    "assert _key_from_message(message, 'subject') == 'supervised|medium|bike|2013|0|0.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because MinIO uses prefixes only to filter items, so do I, even with local storage. These are ordered, so we build a prefix from a dictionary, as long as each value in the ordered list is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1|2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_prefix_from('subject', dict(purpose=1, size=2, year=2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1|2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_prefix_from('treatment', dict(purpose=1, name=2, foo=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list('abcdefg')\n",
    "d = dict(a=1, b=2, d=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for k in a:\n",
    "    if not k in d:\n",
    "        break\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I now know is the put operation needs to serialize or compress an item or group of items, and that the get operations needs to deserialize or decompress an item or group of items.\n",
    "\n",
    "I know that goofing with the data (mocking it midway) shows me more of what I need to do on the types.\n",
    "\n",
    "I also know that the old school stuff with remote vs local data is a thing. I also know that there probably needs to be a different kind of object storage for meta data vs data. Meaning, subjects and modes get stored in some state of compression/raw/whatever, and that the meta data is something that has types.\n",
    "\n",
    "Also, there could be different formats of the same subject, which is different than versions. A version on a model just increments. I treated it with X, and it gets a version 0.2.1.\n",
    "\n",
    "* type relations\n",
    "* learning type\n",
    "* pull-through storage\n",
    "* processing on the pull through (say tab-separated to CSV or even a preprocessing pipeline...more tbd)\n",
    "* attribute updates\n",
    "* more subject raw data\n",
    "* more treatment raw/fake data\n",
    "* more evaluation raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_RESULTS = -1\n",
    "ALPHABETIC_SORT = 'todo'\n",
    "RANDOM_SORT = 'todo'\n",
    "\n",
    "def _alphabetic_sort(seq):\n",
    "    \"\"\"Dictionaries need a key\"\"\"\n",
    "    try:\n",
    "        return sorted(seq)\n",
    "    except:\n",
    "        return seq\n",
    "\n",
    "def _random_sample(seq):\n",
    "    seq = list(seq)\n",
    "    k = len(seq)\n",
    "    return random.sample(seq, k=k)\n",
    "\n",
    "def _by_dictionary_key(seq, k):\n",
    "    seq = list(seq)\n",
    "    return sorted(seq, key=lambda d: d.get(k, 0))\n",
    "\n",
    "def _by_name(seq):\n",
    "    return _by_dictionary_key(seq, 'name')\n",
    "\n",
    "SORT_MAP = dict(\n",
    "    alphabetic = _alphabetic_sort,\n",
    "    name = _by_name,\n",
    "    random = _random_sample,\n",
    ")\n",
    "DEFAULT_SORT = 'name'\n",
    "_item_types_list = [\n",
    "    'subject', 'treatment', 'model', 'evaluation',\n",
    "    'expecation', 'invocation'\n",
    "]\n",
    "ITEMS_MAP = {k:f'{k}s' for k in _item_types_list}\n",
    "DEFAULT_ITEM = 'model'\n",
    "    \n",
    "def _get_sort(o):\n",
    "    if callable(o): return o\n",
    "    if o in SORT_MAP: return SORT_MAP[o]\n",
    "    return SORT_MAP[DEFAULT_SORT]\n",
    "\n",
    "def _label_filter_for(labels, limit):\n",
    "    pass\n",
    "\n",
    "def _get_items_with(bucket, item_filter):\n",
    "    # Do extract here...\n",
    "    # return list('abcdefghijklmnopqrstuvwxyz')\n",
    "    return [{'name': k} for k in random.sample(list('abcdef'), k=6)]\n",
    "\n",
    "def _get_objects_by_label(\n",
    "    labels, bucket, limit=ALL_RESULTS, sort=DEFAULT_SORT, **kw):\n",
    "    \n",
    "    bucket = ITEMS_MAP.get(bucket, DEFAULT_ITEM)\n",
    "    item_filter = _label_filter_for(labels, limit)\n",
    "    items = _get_items_with(bucket, item_filter)\n",
    "\n",
    "    sorter = _get_sort(sort)\n",
    "    items = sorter(items)\n",
    "\n",
    "    if limit == ALL_RESULTS: return items\n",
    "    return items[:limit]\n",
    "\n",
    "def get_subjects(labels, **kw):\n",
    "    return _get_objects_by_label(labels, 'subject', **kw)\n",
    "\n",
    "def get_treatments(labels, **kw):\n",
    "    return _get_objects_by_label(labels, 'treatment', **kw)\n",
    "\n",
    "def get_models(labels, **kw):\n",
    "    return _get_objects_by_label(labels, 'model', **kw)\n",
    "\n",
    "def get_evaluations(labels, **kw):\n",
    "    return _get_objects_by_label(labels, 'evaluation', **kw)\n",
    "\n",
    "def get_expectations(labels, **kw):\n",
    "    return _get_objects_by_label(labels, 'expectation', **kw)\n",
    "\n",
    "def get_invocations(labels, **kw):\n",
    "    return _get_objects_by_label(labels, 'invocation', **kw)\n",
    "\n",
    "def dispatch(subject, treatment, evaluations):\n",
    "    print(\"Dispatch\")\n",
    "#     print(f\"Dispatch: subject: {subject}, treatment: {treatment}, evaluations: {evaluations}\")\n",
    "\n",
    "def get_trained(subject, treatment, evaluations):\n",
    "    print(\"Find\", end=\"...\")\n",
    "#     print(f\"Find: subject: {subject}, treatment: {treatment}, evaluations: {evaluations}\")\n",
    "\n",
    "def get_or_train(subject, treatment, evaluations):\n",
    "    get_trained(subject, treatment, evaluations)\n",
    "    if random.random() > 0.5:\n",
    "        dispatch(subject, treatment, evaluations)\n",
    "    else:\n",
    "        print('Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = _get_objects_by_label('labels', 'bucket', limit=3)\n",
    "assert len(found) == 3\n",
    "names = [e['name'] for e in found]\n",
    "assert sorted(names) == names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = get_subjects(\n",
    "    ['research', 'supervised', 'small'],\n",
    "    limit=2, sort='random'\n",
    ")\n",
    "\n",
    "treatments = get_treatments(['common', 'supervised', 'first_pass'], limit=1)\n",
    "evaluations = get_evaluations(['supervised'], limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(subjects, treatments, evaluations):\n",
    "    for subject in subjects:\n",
    "        for treatment in treatments:\n",
    "            get_or_train(subject, treatment, evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find...Dispatch\n",
      "Find...Dispatch\n",
      "__________\n",
      "Find...Dispatch\n",
      "Find...Found\n",
      "__________\n",
      "Find...Dispatch\n",
      "Find...Dispatch\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "learner = partial(learn, subjects, treatments, evaluations)\n",
    "for _ in range(3):\n",
    "    learner()\n",
    "    print('_' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_step(name, message=None, status='passed', **kw):\n",
    "    d = {**kw, **dict(name=name, status=status)}\n",
    "    if not message is None: d['message'] = message\n",
    "    return d\n",
    "\n",
    "def _fail_step(name, exception=None, **kw):\n",
    "    return _create_step(name, status='failed', message=str(exception), **kw)\n",
    "\n",
    "def validate_treatment(fn, **kw):\n",
    "    steps = []\n",
    "    advance = lambda e: steps.append(_create_step(e))\n",
    "    try:\n",
    "        current = 'setup class'\n",
    "        cls = fn(**kw)\n",
    "        advance(current)\n",
    "        \n",
    "        current = 'setup model'\n",
    "        model = cls()\n",
    "        advance(current)\n",
    "        \n",
    "        # TODO: check duck typing\n",
    "        return True, steps\n",
    "\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        steps.append(_fail_step(current, exception=e, traceback=exc_traceback))\n",
    "        return False, steps\n",
    "    \n",
    "def format_validation(steps, valid=None):\n",
    "    def filter_dicts(l, key='status', value=None):\n",
    "        return [d for d in l if d.get(key) == value]\n",
    "    def get_key(l, key):\n",
    "        return [e.get(key) for e in l]\n",
    "    def get_paired(l, value):\n",
    "        filtered = filter_dicts(l, value=value)\n",
    "        labeled = get_key(filtered, 'name')\n",
    "        return filtered, labeled\n",
    "\n",
    "    if valid is None: valid = len(filter_dicts(steps, value='failed')) == 0\n",
    "    \n",
    "    def explain_success(step):\n",
    "        name = step.get('name', \"unknown\")\n",
    "        return f\"Success with: {name}\"\n",
    "\n",
    "    def explain_failure(step):\n",
    "        name = step.get('name', \"unknown\")\n",
    "        reason = step.get('message', \"reason unknown\")\n",
    "        return f\"Failing step: {name}. Reason: {reason}.\"\n",
    "\n",
    "    def explain_malformed(step):\n",
    "        name = step.get('name', \"unknown\")\n",
    "        reason = step.get('message')\n",
    "        message = f\"Possible problem with {name}. That status was not set.\"\n",
    "        if not reason is None: message += \" Message: {reason}\"\n",
    "        return message\n",
    "        \n",
    "    def explain_step(step):\n",
    "        status = step.get('status')\n",
    "        if status == 'passed': return explain_success(step)\n",
    "        if status == 'failed': return explain_failure(step)\n",
    "        return explain_malformed(step)\n",
    "    \n",
    "    messages = []\n",
    "\n",
    "    if valid:\n",
    "        _passed_steps, passed_names = get_paired(steps, 'passed')\n",
    "        messages.append(\"Validation was successful.\")\n",
    "        if len(passed_names) == 0:\n",
    "            messages.append(\"No successful steps were reported.\")\n",
    "        else:\n",
    "            messages.append(f\"Steps: {', '.join(passed_names)}\")\n",
    "    else:\n",
    "        messages.append(\"Validation failed.\")\n",
    "        \n",
    "        for step in steps:\n",
    "            messages.append(explain_step(step))\n",
    "\n",
    "        failed_steps = filter_dicts(steps, value='failed')\n",
    "        if len(failed_steps) == 0: messages.append(\"No failing steps were reported.\")\n",
    "\n",
    "    return \"\\n\".join(messages)\n",
    "    \n",
    "def store_treatment(fn, **kw):\n",
    "    valid, steps = validate_treatment(fn, **kw)\n",
    "    if not valid:\n",
    "        print(format_validation(steps, valid=False))\n",
    "        return False, steps\n",
    "    # TODO: extract/enforce more treatment attributes\n",
    "    # TODO: think about object store dependency\n",
    "    # object_store.put(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed.\n",
      "Failing step: setup class. Reason: 'str' object is not callable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " [{'traceback': <traceback at 0x115075900>,\n",
       "   'name': 'setup class',\n",
       "   'status': 'failed',\n",
       "   'message': \"'str' object is not callable\"}])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_treatment('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = lambda: \"This would be the model.\"\n",
    "l1 = lambda **kw: l2\n",
    "store_treatment(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "<class 'AttributeError'>\n",
      "Cool message bro.\n",
      "<traceback object at 0x114852b80>\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "message = None\n",
    "problem = None\n",
    "tback = None\n",
    "try:\n",
    "    raise AttributeError(\"Cool message bro.\")\n",
    "except Exception as e:\n",
    "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    print('!' * 100)\n",
    "    print(exc_type)\n",
    "    print(exc_value)\n",
    "    print(exc_traceback)\n",
    "    print('_' * 100)\n",
    "\n",
    "    tback = exc_traceback\n",
    "    problem = e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although really rough, this is cool:\n",
    "\n",
    "* create a way to validate each type.\n",
    "* use a consistent workflow to validate, report, and store objects\n",
    "* push all the object store details lower\n",
    "\n",
    "What I think happens is I mention something I want to to use, and the system looks for it. If I'm not satisfied, because it's incomplete or I want to improve it, then I either branch from that concept or start something fresh.\n",
    "\n",
    "Maybe I iteratively get a subject into the lab. The reference is there, or I read a paper using a public dataset. I figure out if I want it stored locally, or where, if anywhere. I figure out what the processing whould be done on it. Maybe I have some downsampling tools, maybe I have some EDA tools, maybe I build a pipeline, maybe I just goof with the data in a notebook, informally figuring out how to work with the subject.\n",
    "\n",
    "Meanwhile, there are notes about the dataset, about the tools I'm using, about the papers I'm reading. These should be treated like slips. I'm not very happy with the documentation tools in nbdev yet, but maybe I figure it out well enough to add a slip annotation to a notebook. The idea is the cell becomes the slip, and I just leave the full reference or a citation at the bottom.\n",
    "\n",
    "So, it's iterative, and this interface is about right, yes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = traceback.extract_tb(tback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<FrameSummary file <ipython-input-297-0baecb16672e>, line 5 in <module>>]\n"
     ]
    }
   ],
   "source": [
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cool message bro.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\n",
    "    dict(name='a', status='passed'),\n",
    "    dict(name='b', status='passed'),\n",
    "    dict(name='c', status='failed', message='Explain problems with c'),\n",
    "    dict(name='d', status='passed'),\n",
    "    dict(name='e'),\n",
    "    dict(name='f', status='failed'),\n",
    "    dict(status='failed', message=\"Something went wrong in a malformed step\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation was successful.\n",
      "Steps: a, b, d\n"
     ]
    }
   ],
   "source": [
    "print(format_validation(a, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed.\n",
      "Success with: a\n",
      "Success with: b\n",
      "Failing step: c. Reason: Explain problems with c.\n",
      "Success with: d\n",
      "Possible problem with e. That status was not set.\n",
      "Failing step: f. Reason: reason unknown.\n",
      "Failing step: unknown. Reason: Something went wrong in a malformed step.\n"
     ]
    }
   ],
   "source": [
    "print(format_validation(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed.\n",
      "Possible problem with a. That status was not set.\n",
      "Failing step: b. Reason: reason unknown.\n"
     ]
    }
   ],
   "source": [
    "a = [\n",
    "    {'name': 'a'},\n",
    "    {'name': 'b', 'status': 'failed'},\n",
    "]\n",
    "print(format_validation(a, False))\n",
    "# print(format_validation([{'name': 'a',}, {'name': 'b', status='failed'}], False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed.\n",
      "No failing steps were reported.\n"
     ]
    }
   ],
   "source": [
    "print(format_validation([], False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation was successful.\n",
      "No successful steps were reported.\n"
     ]
    }
   ],
   "source": [
    "print(format_validation([], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def _list_paths(**kw):\n",
    "    \"\"\"In the local object store, a directory is a bucket.\n",
    "    Find all directories, given an optional filter.\"\"\"\n",
    "    r = _root(**kw)\n",
    "    p = _bucket_filter(**kw)\n",
    "    return [f for f in r.iterdir() if f.is_dir() and p.match(f.name)]\n",
    "\n",
    "def list_buckets(**kw):\n",
    "    \"\"\"Convert found directories to just their names.\"\"\"\n",
    "    \n",
    "    # TODO: Consider expanding this to a sequence of bucket types,\n",
    "    # after working through this interface and the MinIO one.\n",
    "    return [p.name for p in _list_paths(**kw)]\n",
    "\n",
    "def _get_slice(seq, key, default=None):\n",
    "    \"\"\"Get key from a sequence, returning a default.\n",
    "    This is the same as d.get(key, default), but allowing\n",
    "    a slice instead of just a key (so _get_slice(a, slice(2:4))\n",
    "    works).\n",
    "    Note: I use this here on sequences, rather than maps,\n",
    "    a quick way to say something like a[0] when a is\n",
    "    potentially empty.\"\"\"\n",
    "    d = dict(enumerate(seq))\n",
    "    return d.get(key, default)\n",
    "\n",
    "def _first_path(**kw):\n",
    "    \"\"\"What's the first path filtered from root, default None.\"\"\"\n",
    "    return _get_slice(_list_paths(**kw), 0)\n",
    "\n",
    "def _first_bucket(**kw):\n",
    "    \"What's the first directory name filtered from root, default None.\"\n",
    "    return _get_slice(list_buckets(**kw), 0)\n",
    "\n",
    "def _exists(**kw):\n",
    "    \"\"\"Does a directory exist, given a filter?\"\"\"\n",
    "    found = _first_path(**kw)\n",
    "    return not found is None and found.exists()\n",
    "\n",
    "def _is_empty(**kw):\n",
    "    \"\"\"Are there contents inside the directory?\n",
    "    Returns True (is empty) when the directory doesn't exist.\"\"\"\n",
    "    if not _exists(**kw): return True\n",
    "    return len(list(_first_path(**kw).glob('*'))) == 0\n",
    "\n",
    "def _can_remove_bucket(**kw):\n",
    "    kw = merge_config(**kw)\n",
    "    if _is_empty(**kw): return True\n",
    "    if kw.get('force', False): return True\n",
    "    return False\n",
    "\n",
    "def _full_path(**kw):\n",
    "    return root(**kw)/name(**kw)\n",
    "    \n",
    "def find_or_create_bucket(**kw):\n",
    "    path = _full_path(**kw)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path.name\n",
    "\n",
    "def remove_bucket(**kw):\n",
    "    if not _can_remove_bucket(**kw): return False\n",
    "    try:\n",
    "        path = _full_path(**kw)\n",
    "        shutil.rmtree(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface\n",
    "\n",
    "Anything not in the interface gets an underscore prefix. This is because this file is meant to be used as a namespace:\n",
    "\n",
    "    import lab.train.local_object_store as object_store\n",
    "    object_store.list_buckets()\n",
    "    \n",
    "Then, when I get my config and MinIO where I want it:\n",
    "\n",
    "    import lab.train.minio_object_store as object_store\n",
    "    object_store.list_buckets()\n",
    "\n",
    "Now I've got a remote and sharable object store that can version my work the way I've wanted to use it. If I'm working on a model, or I've dispatched a model to be trained, the object store will be updated when the work is ready.\n",
    "\n",
    "The interface is:\n",
    "\n",
    "    list_buckets\n",
    "    find_or_create_bucket\n",
    "    remove_bucket\n",
    "    find_items\n",
    "    put\n",
    "    get\n",
    "    get_stats\n",
    "    copy\n",
    "    remove\n",
    "\n",
    "I'm working out in here what the signature is for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(list_buckets(), list)\n",
    "\n",
    "remove_bucket(name='junk', force=True)\n",
    "assert _full_path(name='junk') == Path(ROOT)/'junk'\n",
    "assert not _exists(name='junk')\n",
    "assert _first_bucket(name='junk') is None\n",
    "assert _first_path(name='junk') is None\n",
    "assert list_buckets(name='junk') == []\n",
    "assert _can_remove_bucket(name='junk')\n",
    "\n",
    "find_or_create_bucket(name='junk')\n",
    "\n",
    "assert len(list_buckets(name='junk')) == 1\n",
    "assert _is_empty(name='junk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucket management is kind of cool. We can force a removal of a bucket, even with contents in it, but I'll create objects in the bucket first, then I can come back to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_filter(**kw):\n",
    "    kw = merge_config(**kw)\n",
    "    return '*' # TODO\n",
    "    \n",
    "def find_items(**kw):\n",
    "    path = _first_path(**kw)\n",
    "    if path is None: return []\n",
    "    return [o for o in path.glob(item_filter(**kw)) if o.is_file()]\n",
    "\n",
    "def put(**kw):\n",
    "    pass\n",
    "\n",
    "def get(**kw):\n",
    "    pass\n",
    "\n",
    "def get_stats(**kw):\n",
    "    pass\n",
    "\n",
    "def copy(**kw):\n",
    "    pass\n",
    "\n",
    "def remove(**kw):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* what is the thing? (open file reference, filename, url, content)\n",
    "* item filter syntax (include white list above)\n",
    "* underscore second-class functions (still exported, but not meant for the main interface)\n",
    "* put an object by its reference\n",
    "* deal with pickle or better-than-pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/junk')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_first_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
